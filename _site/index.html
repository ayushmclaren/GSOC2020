<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="QBLD - Quantile Regression for Binary Longitudinal Data" />


<title>Google Summer of Code 2020, Ayush Agarwal</title>

<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

/* A workaround for https://github.com/jgm/pandoc/issues/4278 */
a.sourceLine {
  pointer-events: auto;
}

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<link rel="stylesheet" href="index_files/style.css" type="text/css" />





</head>

<body>




<div class="entry-header">
  <div class="entry-image">
  </div><!-- /.entry-image -->
</div><!-- /.entry-header -->
<div id="main" role="main">
<article class="hentry">
<header class="header-title">
<div class="header-title-wrap">
<h1 class="title toc-ignore entry-title">Google Summer of Code 2020, Ayush Agarwal</h1>
<h3 class="author">QBLD - Quantile Regression for Binary Longitudinal Data</h3>
</div><!-- /.header-title-wrap -->
</header>


<div class="entry-content">

<div id="project-details" class="section level2">
<h2>Project Details</h2>
<hr />
<div id="project-title-qbld-quantile-regression-for-binary-longitudinal-data" class="section level3">
<h3>Project Title : QBLD: Quantile Regression for Binary Longitudinal Data</h3>
</div>
<div id="project-short-title-qbld" class="section level3">
<h3>Project short title : QBLD</h3>
</div>
<div id="project-url-xxxxx" class="section level3">
<h3>Project url : xxxxx</h3>
</div>
</div>
<div id="about-me" class="section level2">
<h2>About Me</h2>
<hr />
<p>I am a junior undergraduate in the Department of Mathematics and Statistics at IIT Kanpur and have experience in the field of Markov Chain Monte Carlo (MCMC) through both courses and projects. I am crediting post-graduate courses on MCMC and Bayesian Econometrics, where I am learning the theoretical as well as implementation details of the field.<br />
I am proficient in R and have gained considerable implementation experience in the language through projects and implementation assignment in courses. I have implemented a variety of mcmc and time series models in research projects on <em>Gaussian Mixture Models</em> and <em>Bayesian Logisitic Regression</em> under Prof. Dootika Vats and <a href="https://github.com/ayushmclaren/Time-Series-and-Forecasting"><em>Time Series Analysis and Forecasting</em></a> under Prof. Amit Mitra of IIT Kanpur. I hold the position of coordinator of Stamatics, the mathematics and statistics society of IIT Kanpur. Additionally, I am mentoring sophomores and freshers on Probabilistic Machine Learning and Bayesian MCMC models as part of a project under statistics and the computer science association of IIT Kanpur.</p>
<p><strong>Relevant Experience</strong></p>
<div id="gaussian-mixture-models--" class="section level3">
<h3>Gaussian Mixture Models -</h3>
<p>Mentor :- Prof. Dootika Vats</p>
<ul>
<li>Implemented clustering mechanism in R using the Expectation-Maximization Algorithm.</li>
<li>Using the AIC Loss function and Cross-Validation techniques, outputting and plotting final clustering.</li>
</ul>
<p>GitHub link - <a href="https://github.com/ayushmclaren/Gaussian-Mixture-Models" class="uri">https://github.com/ayushmclaren/Gaussian-Mixture-Models</a></p>
</div>
<div id="bayesian-logistic-regression--" class="section level3">
<h3>Bayesian Logistic Regression -</h3>
<p>Mentor :- Prof. Dootika Vats</p>
<ul>
<li>Implemented a Bayesian Logistic Regression model in R for a Bernoulli likelihood.</li>
<li>Analysed the obtained fits using Trace, Time, and Auto-Correlation plots to further help in tuning.</li>
</ul>
<p>GitHub link - <a href="https://github.com/ayushmclaren/Bayesian-Logisitic-Regression" class="uri">https://github.com/ayushmclaren/Bayesian-Logisitic-Regression</a></p>
</div>
<div id="time-series-analysis-and-forecasting--" class="section level3">
<h3>Time Series Analysis and Forecasting -</h3>
<p>Mentor :- Prof. Amit Mitra</p>
<ul>
<li>Implemented a SARIMA time series model in python for Monthly Rainfall Data of the last 114 years in India.</li>
<li>Using the PACF Interpolation techniques, missing values were internally interpolated to maintain the full data.</li>
<li>Using the Dickey-Fuller Test, data was stationarized at first to implement the SARIMA model.</li>
<li>Using the Box-Jenkins Algorithm, predictive forecasting for the next month was achieved.</li>
</ul>
<p>GitHub link - <a href="https://github.com/ayushmclaren/Time-Series-and-Forecasting" class="uri">https://github.com/ayushmclaren/Time-Series-and-Forecasting</a></p>
</div>
<div id="relevant-coursework" class="section level3">
<h3>Relevant Coursework :</h3>
<p><strong>Markov Chain Monte Carlo</strong> | <strong>Time Series Analysis</strong><br />
<strong>Bayesian Econometrics</strong> | <strong>Quantitative Methods</strong><br />
<strong>Statistical Simulation and Data Analysis</strong> | <strong>Probability and Statistics</strong></p>
</div>
</div>
<div id="contact-information" class="section level2">
<h2>Contact Information :</h2>
<hr />
<p><strong>Email</strong> : <a href="mailto:ayush.agarwal50@gmail.com" class="email">ayush.agarwal50@gmail.com</a>,  <a href="mailto:ayuag@iitk.ac.in" class="email">ayuag@iitk.ac.in</a><br />
<strong>GitHub handle</strong> : <a href="http://github.com/ayushmclaren">ayushmclaren</a><br />
<strong>Postal Address</strong> : A 302 Hall 5, IIT Kanpur, Uttar Pradesh, 208016<br />
<strong>Telephone</strong> : +91 9815139259 <strong>Skype Id</strong> : Ayush Agarwal</p>
</div>
<div id="student-affiliation" class="section level2">
<h2>Student Affiliation</h2>
<hr />
<p><strong>Institution </strong> : Indian Institute of Technology, Kanpur<br />
<strong>Program</strong> : Bachelor of Science, Mathematics and Statistics<br />
<strong>Stage of Completion</strong> : Currently in my <span class="math inline">\(3^{rd}\)</span> (penultimate) year<br />
<strong>Contact to verify</strong> : Prof. Mohammad Arshad Rahman (<a href="mailto:marshad@iitk.ac.in" class="email">marshad@iitk.ac.in</a>) / <a href="https://www.iitk.ac.in/dosa/">Dean, Students Affairs, IIT Kanpur</a></p>
</div>
<div id="mentors" class="section level2">
<h2>Mentors</h2>
<hr />
<p><strong>Evaluating mentor</strong> : Prof. Dootika Vats (<a href="mailto:dootika@iitk.ac.in" class="email">dootika@iitk.ac.in</a>)</p>
<p><strong>Co-mentor</strong> : Prof. Adam Maidman ()</p>
</div>
<div id="background" class="section level2">
<h2>Background</h2>
<p>Panel data have been attractive for understanding behavior and dynamics, the modeling complexities involved have moved attention away from the data’s unique capacities. Modeling features such as a binary outcome variable or a quantile analysis, which are relatively straightforward to implement with cross-sectional data, are challenging and computationally burdensome for panel data. However, these features are important as they allow for the modeling of probabilities and lead to a richer view of how the covariates influence the outcome variable. For limited dependent variables, the concern is modeling the latent utility differential in the quantile framework, since the response variable takes limited values and does not yield continuous quantiles.</p>
<p>This project follows Rahman and Vossmeyer (2019) as its motivating literature(hereon referred to as master literature), and contributes to the three literatures by extending the various methodologies to a hierarchical Bayesian quantile regression model for binary longitudinal data (QBLD) and proposing a Markov chain Monte Carlo (MCMC) algorithm to estimate the model. The model handles both common (fixed) and individual-specific (random) parameters (commonly referred to as mixed effects in statistics). The algorithm implements a blocking procedure that is computationally efficient and the distributions involved allow for straightforward calculations of covariate effects.</p>
<p><strong>The Model</strong></p>
<p>The QBLD model can be conveniently expressed in the latent variable formulation (Albert &amp; Chib, 1993) as follows:</p>
<p><span class="math display">\[ z_{it} = x_{it}^{&#39;}\beta + s_{it}^{&#39;}\alpha_{i} + \epsilon_{it}   \hspace{35 pt} \forall i = 1,...,n; \hspace{5 pt} t = 1,...,T_{i} \]</span> <span class="math display">\[ 
y_{it} = 
     \begin{cases}
       \text{1}  &amp;\quad {if \hspace{5 pt} z_{it}\gt 0} \\
       \text{0} &amp;\quad\text{otherwise,} \\
     \end{cases}
\]</span></p>
<p>where the latent variable <span class="math inline">\(z_{it}\)</span> denotes the value of <span class="math inline">\(z\)</span> at the <span class="math inline">\(t^{th}\)</span> time period for the <span class="math inline">\(i^{th}\)</span> individual, <span class="math inline">\(x_{it}^{&#39;}\)</span> is a <span class="math inline">\(1 × k\)</span> vector of explanatory variables, <span class="math inline">\(\beta\)</span> is <span class="math inline">\(k × 1\)</span> vector of common parameters, <span class="math inline">\(s_{it}^{&#39;}\)</span> is a <span class="math inline">\(1 × l\)</span> vector of covariates that have individual-specific effects, <span class="math inline">\(\alpha_{i}\)</span> is an <span class="math inline">\(l × 1\)</span> vector of individual-specific parameters, and <span class="math inline">\(\epsilon_{it}\)</span> is the error term assumed to be independently and identically distributed (i.i.d.) as <span class="math inline">\(AL(0, 1, p)\)</span> with <span class="math inline">\(Q_{\epsilon_{it}}(p| x_{it}, \alpha_{i}) = 0\)</span>.</p>
<p>While working directly with the AL density is an option, the resulting posterior will not yield the full set of tractable conditional distributions necessary for a Gibbs sampler. Thus, we utilize the normal-exponential mixture representation of the AL distribution, presented in Kozumi and Kobayashi (2011).</p>
<p>The appropriate priors are chosen and a blocked Gibbs sampler is implemented as described in Rahman and Vossmeyer (2019).</p>
</div>
<div id="related-work" class="section level2">
<h2>Related Work</h2>
<p>Quantile regression has been implemented in binary data models (Benoit &amp; Poel, 2012; Kordas, 2006), ordered data models (Alhamzawi &amp; Ali, 2018; Rahman, 2016), count data models (Machado &amp; Silva, 2005), and censored data models (Harding &amp; Lamarche, 2012; Portnoy, 2003). The latter of these papers discusses the issues associated with solely focusing on fixed effects estimators and highlights the usefulness of allowing for a flexible specification of individual heterogeneity associated with covariates, also of interest in this framework.</p>
<p>To the best of my knowledge, several quantile regression packages already exist namely, <code>quantreg</code>, <code>BayesQR</code>, <code>MCMCpack</code>, <code>bqror</code>. However, these packages are either based on freuqentist frameworks or incapable of handling mixed effects BLD. <code>BayesQR</code> can handle a BLD framework, albeit using a Metropolis-within-Gibbs sampler and can only account for random effects. The proposed package aims to complement the existing packages and widen the scope of quantile regression framework to heterogenous binary longitudinal data. Similarly <code>brq</code> package implements the idea of Bayesian adaptive Lasso quantile regression.</p>
<p>In a recent Bayesian paper, Luo, Lian, and Tian (2012) develop a hierarchical model to estimate the parameters of conditional quantile functions with random effects. The authors do so by adopting an asymmetric Laplace (AL) distribution for the residual errors and suitable prior distributions for the parameters. Sampler for AL distribution is implemented in the <code>ald</code> package.</p>
<p>However, directly using the AL distribution does not yield tractable conditional densities for all of the parameters and hence a combination of Metropolis-Hastings (MH) and Gibbs sampling is required for model estimation. The use of the MH algorithm may require tuning at each quantile. To overcome this limitation, I will implement a full Gibbs sampling algorithm that utilizes the normal-exponential mixture representation of the AL distribution. The other significant point of difference is that the model handles both common (fixed) and individual-specific (random) parameters.</p>
</div>
<div id="expected-impact-and-usage" class="section level2">
<h2>Expected Impact and Usage</h2>
<p>The paper on, “Estimation and Applications of Quantile Regression for Binary Longitudinal Data” by Rahman, and Vossmeyer (2019) develops a framework for quantile regression in binary longitudinal data settings. A novel Markov chain Monte Carlo (MCMC) method is designed to fit the model and its computational efficiency is demonstrated. Panel data have been attractive to reserachers for understanding behavior and dynamics. The paper already has 6 citations in less than 6 months, demonstrating an interest in the field. An open source code in R is not available yet, however, a rudimentary implementation in MATLAB has been achieved. Availability of an open source tool will further help researchers take advantage of the theoretical development.</p>
<p>Researchers, and students in fields related to Quantile Regression, Econometrics, and Policy making that use R will be benefited by the new tools generated with this project. The set of tools to be created will facilitate deeper insights into panel data in a binary response variable environment, that otherwise will require extensive analyses in distinct software platforms. These tools will help in accounting for hetergoenity. Heterogeneity implies that response may differ in terms of certain unmeasured variables that affect the probability of the outcome. Users will find in this package an important set of tools that will help them to make informed decisions.</p>
<p><strong>Usage (Till date)</strong></p>
<p>The methodology has been applied to study female labor force participation and home ownership in the United States. The results offer new insights at the various quantiles, which are of interest to policymakers and researchers alike. The datasets for both the examples will be a part of the package and will help in demonstartion of the tools through a R vignette.</p>
<p><strong>CRAN release</strong></p>
<p>The <code>qbld</code> package will be submitted to the authors for a peer review. After its hopefully smooth acceptance, I will submit it to CRAN under GPL(&gt;2). A developmental version of the package will be hosted on GitHub and tested for future releases.</p>
</div>
<div id="coding-plan-and-methods" class="section level2">
<h2>Coding Plan and Methods</h2>
<hr />
<div id="asymmetric-laplace-distribution" class="section level3">
<h3>Asymmetric Laplace distribution</h3>
<p>The current implementation of the random number generator function <code>rALD</code> in the <code>ald</code> package uses the three parameter Asymmetric Laplace Distribution defined in Koenker and Machado (1999). I will implement a more efficient method by utilizing the normal-exponential mixture representation of the AL distribution, presented in Kozumi and Kobayashi (2011). This method only requires the skewness paramter <em>p</em> as the input and will make use of <code>rnorm</code> and <code>rexp</code> functions.</p>
<p><span class="math display">\[ \epsilon_{it} = w_{it}* \theta + \tau * \sqrt{w_{it}}*u_{it} \hspace{20 pt} \forall i = 1,...,n; \hspace{5 pt} t = 1,...,T_{i} \]</span> where <span class="math inline">\(u_{it} \sim N(0,1)\)</span> is mutually independent of <span class="math inline">\(w_{it} ∼ \epsilon(1)\)</span> with <span class="math inline">\(\epsilon\)</span> representing an exponential distribution and the constants <span class="math inline">\(\theta = \frac{1-2p}{p(1-p)}\)</span> and <span class="math inline">\(\tau = \sqrt{\frac{2}{p(1-p)}}\)</span>. Supporting functions analogous to <code>dALD</code>, <code>qALD</code> for density and quantile calculations respectively will also be similarly implemented.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">rALD &lt;-<span class="st"> </span><span class="cf">function</span>(n,p) <span class="co">#random number generator</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2">{</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">  <span class="co"># skewness parameter and number of samples as the input.</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">  <span class="co"># the output follows a AL(0,1,p) distribution. (normalized for interpretation)</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">qALD &lt;-<span class="st"> </span><span class="cf">function</span>(prob,p,tail) <span class="co">#quantile function</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8">{</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">  <span class="co">#tail is logical variable to decide P(X&lt;=prob) or P(X&gt;prob) </span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1-11" data-line-number="11"></a>
<a class="sourceLine" id="cb1-12" data-line-number="12">dALD &lt;-<span class="st"> </span><span class="cf">function</span>(y,p) <span class="co">#gives the density</span></a>
<a class="sourceLine" id="cb1-13" data-line-number="13">{</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">  <span class="co">#evaluates density of AL(0,1,p) at y</span></a>
<a class="sourceLine" id="cb1-15" data-line-number="15">}</a></code></pre></div>
</div>
<div id="the-gibbs-sampler" class="section level3">
<h3>The Gibbs Sampler</h3>
<p>Following the master literature, I will implement a blocked Gibbs sampler for the QBLD framework which can handle heterogenity in both common (fixed) and individual-specific(random) parameters. The Blocking procedure is essential for efficiency and reducing computational load. This blocked approach also significantly improves the mixing properties of the Markov chain. The success of these blocking techniques can be found in J.S. Liu (1994), Chib and Carlin (1999), and Chib and Jeliazkov (2006). The functions to sample from the conditional distributions will be set up generically for a wide scope usage outside the model.</p>
<p>To improve the speed of the routine, the Markov Chain Monte Carlo (MCMC) part of the algorithm will be programmed in RCpp and be called from within the function in R.</p>
<p>I have also identified and plan to solve the following additional roadblocks -</p>
<p><strong>Stacking of individual data</strong> -</p>
<p>Since this is a panel data setting, Longitudinal data models often involve a moderately large amount of data, so it is important to take advantage of any opportunity to reduce the computational burden. One such trick is to stack the model for each individual <em>i</em> for all its time points and account for the covariates as one matrix rather than covariate vectors. (Hendricks, Koenker, &amp; Poirier, 1979).</p>
<p><strong>Transposing and Inverting high-dimensional matrices</strong> -</p>
<p>Since the sampler involves calculation and storage of transpose and inverse of large size matrices at repeated intermediate steps, I will offload these functions to RCpp for faster implementation. Cpp is also known to be better at handling the machine tolerance issues than R. Methods to avoid this issue also includes alternative algorithms for sampling from a Multivariate Normal and Using a Non-blocked sampler decribed later.</p>
<p><strong>Penalization and priors on <span class="math inline">\(\beta\)</span> </strong> -</p>
<p>The form of the prior distribution on <span class="math inline">\(\beta\)</span> holds a penalty interpretation on the quantile loss function. A normal prior on <span class="math inline">\(\beta\)</span> implies a <span class="math inline">\(l_2\)</span> penalty and has been used in Luo et al. (2012). One may also employ a Laplace prior distribution on laplace that imposes <span class="math inline">\(l_1\)</span> penalization, as used in Alhamzawi and Ali (2018). I will try to efficiently implement the sampler for latter case giving the user a choice in terms of the priors. In case it is not specified, the deafult will be the normal prior.</p>
<p><strong>Pre-processing of the data</strong> -</p>
<p>The input data by the user will have to be cleaned, by reshaping, deleting or adding rows and columns, removing or intrapolating the missing data cells. I will implement a generic function that will clean the data and mould it into a QBLD format that can be used by the algorithm. The function can be implemented generically for a much larger use case. In case the data cannot be processed, the user will be given appropriate warnings to ensure that the data can be manually processed by the user and fed in the correct format to the sampler.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co"># Returns an object of specified classes that represents a quantile regression fit</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2">qbld.gibbs &lt;-<span class="st"> </span><span class="cf">function</span>(formula,tau,data,<span class="dt">subset=</span>data,method,<span class="dt">summary=</span><span class="ot">TRUE</span>,<span class="dt">blocking =</span> <span class="ot">TRUE</span>, <span class="dt">clean =</span> <span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb2-3" data-line-number="3">{</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">  <span class="co">#formula specifies y~x.</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">  <span class="co">#tau specifies the quantiles to be estimated</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6">  <span class="co">#data is the data.frame to work upon, subset can be specified if the whole data is not to be used.</span></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">  <span class="co">#method is default at l2 with normal prior on beta, may be used to switvh to l1 and laplace</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8">  <span class="co">#summary will specify the object type returned, when true user may be able to call the summary function afterwards</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9">  <span class="co">#blocking may be specified, clean will pre-process the data (maybe implemented outside the function)</span></a>
<a class="sourceLine" id="cb2-10" data-line-number="10">}</a></code></pre></div>
</div>
<div id="handling-categorical-variables" class="section level3">
<h3>Handling Categorical Variables</h3>
<p>It is imperative that categorical variables be handled correctly, because if the algorithm does not employ any checks and treats the categorical variables as normal integers the sampler will inadvertently lead to wrong solutions. Since, this sampler works for the QBLD setting, categorical variables are highly common in these datasets. The design matrix for the model will be needed to be transformed and augmented properly depending on the number of factor variables. This should be done under the hood as the user must be able to specify the formula generically as in for instance, <code>lm(y~x)</code> can handle both continous and factor regressors without user interference. I plan to take this into account and employ a factor variable checker, one of which could be made using <code>as.factor</code> function so that the algorithm gets to the right design matrix. This function can be implemented generically for independent usage in multiple data settings including QBLD.</p>
</div>
<div id="blocked-vs-non-blocked-sampling" class="section level3">
<h3>Blocked vs Non-Blocked Sampling</h3>
<p>Blocking technique is preferred over the Non-blocking counterpart because is provides with a better autocorelation structure and relatively faster mixing. However, the blocking procedure employs inverting large matrices which may be too computationally burdensome. I plan to also give the blocking vs non-blocking samplers as an additional choice for the user.</p>
<p>However, Since the non-blocked version involves inverting only diagonal matrices which is computationally a lot easier, I plan to implement a method for the algorithm to decide when is it computationally feasible to switch to a non-blocked sampler despite giving slightly more autocorrelated samples without the user having to decide or wait longer for the results. One generic way to bypass this problem is to switch beyond a specified dimension of the covariate vector.</p>
<p>A second method involves the use of alternative algorithms to <em>sample from the mutivariate normal</em> which do not require such inverse calculations. I plan to try and implement one of the following two algorithms, (with the effort to find more efficient methods)</p>
<ul>
<li>Fast sampling, Anirban Bhattacharya (2016)</li>
<li>Perturbation-Optimization, François Orieu (2013)</li>
</ul>
<p>These algorithms are specific to a class of normals having desired structure, and avoid the inversion of matrices and can significantly improve efficiency of the sampler. Since, sampling from a multivariate normal is an essential central step in this Gibbs sampler, this will speed up the algorithm significantly. I will also try to implement the samplers genrically for usage outside the present setting.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">fast_rmvn &lt;-<span class="st"> </span><span class="cf">function</span>(n,phi,dmat,method) <span class="co">#fast sample from MVN</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">{</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">  <span class="co">#phi is a decomposition term as specified in the algorithm</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">  <span class="co">#dmat is a decomposition term as specified in the algorithm</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">  <span class="co">#method will employ different methods to solve system of linear equations, SOR,Gauss-Seidel are examples.</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">}</a></code></pre></div>
</div>
<div id="sampling-from-univariatemultivariate-truncated-normal" class="section level3">
<h3>Sampling from Univariate/Multivariate Truncated Normal</h3>
<p>The Gibbs sampler requires to sample from a Truncated Multivariate Normal distribution multiple times across runs and thus it is essential to look at its implementation. In the master literature, algorithm samples <span class="math inline">\(z_{i} \sim TMVN_{B_{i}}(X_{i}\beta + w_{i}\theta,\Omega_{i})\)</span> using a series of conditional posteriors which are univariate truncated normal distributions, where the symbols hold their appropriate meaning. This algorithm was proposed in Geweke(1991).</p>
<p>Geweke’s method is computationally heavy and suffers from slow mixing issues, therefore I will implement other, more efficient methods to sample from a Truncated Multivariate Normal. A few of the algorithms are mentioned :-</p>
<ul>
<li>Exact Hamiltonian Monte Carlo, Pakman (2013), this has been implemented in the <code>tmg</code> package</li>
<li>Minimax Tilting, Botev (2016), this has been implemented in the <code>TruncatedNormal</code> package</li>
<li>Fast Simulation of Hyperplane-Truncated, Cong (2017), no known implementation exists, I will try to implement this efficiently and for generic use.</li>
</ul>
<p><strong>The Generalized Inverse Gamma sampler</strong></p>
<p>I will also try to improve efficiency of the GIG sampler, using the method described in Leydold (2013). An author implementation is already present in <code>GIGrvg</code> package.</p>
</div>
<div id="covariate-effects-and-model-comparison" class="section level3">
<h3>Covariate Effects and Model Comparison</h3>
<p>Covariate effects are a key consideration in model evaluation, forecasting, and policy analysis, yet their dependence on estimation uncertainty has been largely overlooked in previous work. Jeliazkov and Vossmeyer (2016) explores the evidence that reveals that failing to consider estimation variability and relying solely on parameter point estimates may lead to nontrivial biases in covariate effects that can be exacerbated in certain settings, underscoring the pivotal role that estimation uncertainty can play in this context.</p>
<p>I will implement the method described in the said paper genrically for use outside the QBLD framework, this will be used in the final <code>summary</code> table output for the Gibbs output. The probabilities are computed from the AL cdf, allowing for straightforward interpretations of the covariate effects. Implemented in the <code>dALD</code> function I mentioned about earlier.</p>
<p>In the past, marginal effects have often been computed at the average value of the covariates <span class="math inline">\(\bar{x}\)</span> and the parameter point estimate <span class="math inline">\(\hat{\beta}\)</span>. <span class="math inline">\(\bar{x}\)</span> can be meaningless if the set of covariates involves categorical, indicator, or any other kind of discrete variables. The effect can be unrepresentative even with continuous covariates if <span class="math inline">\(\bar{x}\)</span> falls in a low density region (e.g., if that distribution is multimodal), in which case policy recommendations based on <span class="math inline">\(\bar{x}\)</span> may not reflect the effects in the population. To avoid some of the aforementioned issues, covariate effects can be evaluated by the several methods mentioned in the paper.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">cov_effect <span class="op">-</span><span class="st"> </span><span class="cf">function</span>(model) <span class="co">#calculates cov effect </span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">{</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">  <span class="co">#will take the model class object as input and print out the cov_effects </span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4">  <span class="co">#can be extended to work generically for other models, also part of summary function</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5">}</a></code></pre></div>
<p>For model comparison, I will follow standard techniques for longitudinal data models. Specifically, in the <code>summary</code> table output, I will provide the conditional Akaike Information Criterion (AIC), and conditional Bayesian Information Criterion (BIC). The calculations for the conditional AIC and conditional BIC can be found in Greven and Kneib (2010) and Delattre, Lavielle, and Poursat (2014), respectively.</p>
<p>The same will be implemented as genric functions for broader use case.</p>
</div>
<div id="inefficiency-factor" class="section level3">
<h3>Inefficiency Factor</h3>
<p>Final <code>summary</code> table will also include calculations for inefficiency factor (IF) by implementing the Batch Means method employed in Greenberg (2012). I will also try to implement and estimate the probit model for binary longitudinal data (PBLD) using the algorithm presented in Koop, Poirier, and Tobias (2007) and Greenberg (2012) and identical priors for relevant parameters. The results for the QBLD and PBLD models will help us check the conformity with the alreasdy existing models while also highlight the extra information we get with the lower and higher quantiles.</p>
</div>
</div>
<div id="convergence-and-standard-errors" class="section level2">
<h2>Convergence and Standard Errors</h2>
<p>I will also make the output object for Gibbs sampler compatible with <code>mcmcse</code> package. Using the <code>mcse.q</code> function Standard Error will be caluclated and presented to the user in summary format.</p>
<p>The <code>mcmcse</code> package will also further help in testing for convergence in the implemented sampler and will be essential especially while using Non-Blocking technique. The package will also provide confidence regions and Effective Sample Size for further analysis using the inbuilt functions.</p>
<div id="benchmarking-and-profiling" class="section level3">
<h3>Benchmarking and Profiling</h3>
<p>I will use the package <code>profvis</code> for visualizing code profiling data from R. To run code with profiling, wrap the expression in <code>profvis()</code>. By default, this will result in the interactive profile visualizer opening in a web browser.<br />
Examplle implementation -</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">profvis</span>({</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">  g &lt;-<span class="st"> </span><span class="kw">ggplot</span>(diamonds, <span class="kw">aes</span>(carat, price)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">  <span class="kw">print</span>(g)</a>
<a class="sourceLine" id="cb5-4" data-line-number="4">})</a></code></pre></div>
<p><strong>Profiling for cpp</strong> -<br />
I will use the profiling tools in visual studio for cpp functions as <code>profviz</code> doesn’t support profling of individual functions in Rcpp.</p>
<p><strong>Profiling memory</strong> -<br />
Applying functions to a list of Markov chains is memory intensive in R as separate memory is allocated to the output of functions on each call. I will perform profiling for the memory usage to solve this issue. A possible workaround can be to perform all operations on lists in cpp and to clear the working memory of the R session if working in R is unavoidable.</p>
<p>I will create a <em>.Rmd</em> file while optimizing code to store all the approaches for performance optimization, which can be used as a reference for similar problems in the future. I will use the <code>microbenchmark</code> package (which provides an infrastructure to measure and compare the execution time of R expressions accurately) to compare the performance of different approaches.</p>
<p><strong>Possible approaches to improving the performance of bottlenecks</strong> -</p>
<ul>
<li>If the bottleneck is a function in a package, it’s worth looking at other packages that do the same thing. Two good places to start are:<br />
</li>
</ul>
<ol style="list-style-type: decimal">
<li><a href="https://cran.rstudio.com/web/views/">CRAN task views</a>. If there’s a CRAN task view related to the problem domain, it’s worth looking at the packages listed there.<br />
</li>
<li>Reverse dependencies of Rcpp, as listed on its CRAN page. Since these packages use C++, it’s possible to find a solution to the bottleneck written in a higher performance language.<br />
</li>
<li>For Google, trying <em>rseek</em> and for StackOverflow, restricting the search by including the R tag, [R], in the search.</li>
</ol>
<ul>
<li><p>Knowing the exact format of the input so that appropriate, more efficient functions can be used instead of general functions. For example, <code>as.data.frame()</code> is quite slow because it coerces each element into a data frame and then <code>rbind()</code>s them together. If you have a named list with vectors of equal length, you can directly transform it into a data frame. In this case, if you’re able to make strong assumptions about your input, you can write a method that’s about 20x faster than the default.</p></li>
<li><p>Vectorizing the code wherever possible (as loops in a vectorized function are written in C instead of R). Loops in C are much faster because they have much less overhead.</p></li>
<li><p>On further discussion with the mentors, I will look to parallelize the code in R.</p></li>
</ul>
<p>I will perform benchmarking using the package <code>rbenchmark</code>. Given a specification of the benchmarking process (counts of replications, evaluation environment) and an arbitrary number of expressions, <code>benchmark</code> function in <code>rbenchmark</code> evaluates each of the expressions in the specified environment, replicating the evaluation as many times as specified.</p>
</div>
<div id="numerical-instability" class="section level3">
<h3>Numerical Instability</h3>
<p>The Markov Chain sampler used in the algorithm is consistent and eventually converge to the correct target. But since we are performing inverse operations on matrices, the expressions for the inverse might give mathematically inconsistent results. A classic example of this is when repeated checks of positive semi-definiteness of covariance matrices give rise to incositenties in the long run. The user needs to be alerted in such a case and consistency eventually needs to kicks in. Some of the approaches to tackle this issue are<br />
1. Bypassing calculation of inverse by methods described above.<br />
2. Adding an <span class="math inline">\(\epsilon\)</span> term to the values to account fro machine tolerance.<br />
The optimal method to tackle such inconsistencies can be decided after consultation with the mentors.</p>
</div>
<div id="documentation" class="section level3">
<h3>Documentation</h3>
<p>I will use the package <code>roxygen2</code> to maintain and update the documentation. <code>roxygen2</code> generates <em>.md</em> files and creates <code>NAMESPACE</code> with R and Rcpp. The package requires adding roxygen comments to the source code. I will add the necessary roxygen comments to all the functions while in progress to maintain and bring uniformity in all documentation.</p>
<p>I will also add two data sets as part of the package, and publish a vignette which will extensively detail the scope and proper use of the package functions by using the model on the said data sets.</p>
</div>
<div id="visualizations" class="section level3">
<h3>Visualizations</h3>
<p>Visualizations of both table(text) forms and plot forms are also included in the project description and will be a worthy addition for the package, in my opinion.</p>
<p><code>summary.QR</code> will be a package function that summarizes the output of the <code>bayesQR</code> function in an object of class bayesQR.summary. For every estimated beta and sigma, this object will contain the Bayes estimate and the posterior credible interval. The object will also contains other relevant information about the estimation procedure, such as the quantile, the variable names, etc.</p>
<p><code>plot.QR</code> will be a package function that produces quantile plots, traceplots or posterior histograms based on the estimates obtained by the <code>bayesQR</code> function. For quantile plots, note that the more quantiles are estimated with bayesQR, the more detailed the quantile plot will be.</p>
<p>I plan to make the visualizations in the package compatible with <code>ggplot</code> to allow for elegant and sophisticated <em>acf</em> and time series plots for the Markov chains. I also plan on making the function output objects compatible with base package tools such as <code>plot</code> and <code>summary</code> to reduce package dependencies and provide a more useful user experience.</p>
</div>
</div>
<div id="timeline-update-in-the-morning" class="section level2">
<h2>Timeline (update in the morning)</h2>
<hr />
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
PERIOD
</th>
<th style="text-align:left;">
TASK
</th>
<th style="text-align:right;">
MEETINGS
</th>
</tr>
</thead>
<tbody>
<tr grouplength="1">
<td colspan="3" style="background-color: #666; color: #fff;">
<strong>Community Bonding</strong>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
May 4 - June 1 (Pre GSoC period)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Research more on quantile regression related literature and development of second penalised framework. Get familiar with writing R packages with the C++ code and using other tools mentioned. Discussion of some of the roadblocks and plans mentioned in the proposal with mentors.
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr grouplength="5">
<td colspan="3" style="background-color: #666; color: #fff;">
<strong>Phase I: Implementing the sampler</strong>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
June 1 - June 8 (Week 1)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Implementation of Asymmetric Laplace related functions and GIG functions
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
June 8 - June 15 (Week 2)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Implementing the blocked Gibbs sampler code in R followed by cpp implementation. Writing testthat for the implementation, checking for numerical instability and profiling.
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
June 15 - June 22 (Week 3)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Implementing the Non-blocked Gibbs Sampler code in R followed by cpp implementation. Writing testthat for the implementation, checking for numerical instability and profiling.
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
June 22 - June 29 (Week 4)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Handling issues related to categorical variables, checking for numerical instability and test runs of the sampler with new data setting
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
June 29 - July 3 (Phase 1 eval)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Phase one eval prep and cleanup - tying any loose ends up to this point (bug fixes, documentation, incomplete implementation) to submit work for evaluation.
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr grouplength="5">
<td colspan="3" style="background-color: #666; color: #fff;">
<strong>Phase II: Efficiency Improvements</strong>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
July 3 - July 10 (Week 5)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Implementing the blocking vs non-blocking algorithm and testing for efficiency vs accuracy tradeoffs
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
July 10 - July 17 (Week 6)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Implementing Sampling from Univariate/Multivariate Truncated Normal algorithms, Writing testthat for the implementation, checking for numerical instability and profiling.
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
July 17 - July 24 (Week 7)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Implementing the alternative penalization model, test runs and improvements, Implementation of PBLD model functions for comparison
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
July 24 - July 27 (Week 8)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Writing testthat, checking for numerical instability and profiling and writing roxygen comments for documentation for the previous weeks.
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
July 27 - July 31 (Phase 2 eval)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Phase two eval prep and cleanup - tying any loose ends up to this point (bug fixes, documentation, incomplete implementation) to submit work for evaluation.
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr grouplength="5">
<td colspan="3" style="background-color: #666; color: #fff;">
<strong>Phase III: User experience and Integration</strong>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
July 31 - August 7 (Week 9)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Implementing the Covariate Effects and Model Comparison. Writing testthat for the implementation, checking for numerical instability and profiling.
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
Ausgust 7 - August 14 (Week 10)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Implementing the functions for pre-processing the data, and mcmcse support is added, checking for numerical instability and profiling.
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
August 14 - August 21 (Week 11)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Making visualizations in the package compatible with ggplot, base plot and summary functions and write in-package summarizing functions.
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
August 21 - August 24 (Week 12)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Writing roxygen comments for the existing untouched functions in the package. Checking for numerical instability in the remaining functions.
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;border-right:1px solid; padding-left: 2em;" indentlevel="1">
August 24 - August 31 (Final eval)
</td>
<td style="text-align:left;width: 30em; border-right:1px solid;">
Final eval prep and cleanup - tying any loose ends up to this point (bug fixes, documentation, incomplete implementation) to submit work for evaluation.
</td>
<td style="text-align:right;">
3
</td>
</tr>
</tbody>
</table>
</div>
<div id="management-of-coding-project" class="section level2">
<h2>Management of Coding Project</h2>
<hr />
<p>As proposed in my coding plan, I will use <code>testthat</code> for user and code testing. Using <code>testthat</code> will also ensure code submission. I have specifically allocated time slots in my timeline for testing.<br />
I will make multiple minor commits adhering to each new functionality that I add. The minor commits will be followed by a major commit every week, which concludes the successful completion of the task allocated to that week. The minor commits will be helpful for the mentors to track my progress over a week and will dictate the schedule of meetings. My major commits will track my workflow over the entire GSoC period. Missing out on major commits for two continuous week would indicate a problem.</p>
</div>
</div><!-- /.entry-content -->
</article>
</div><!-- /#main -->



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
